# Agent ç³»ç»Ÿæ¶æ„è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç» Agent ç³»ç»Ÿçš„è®¾è®¡å’Œå®ç°ï¼ŒåŒ…æ‹¬ Agent Loopã€æµç”Ÿæˆã€äº‹ä»¶å¤„ç†å’Œæƒé™åè°ƒã€‚

## ğŸ“‹ æ ¸å¿ƒç»„ä»¶æ¦‚è§ˆ

| ç»„ä»¶ | æ–‡ä»¶ä½ç½® | è¡Œæ•° | èŒè´£ |
|------|---------|------|------|
| **AgentPresenter** | `src/main/presenter/agentPresenter/index.ts` | 472 | Agent ç¼–æ’ä¸»å…¥å£ï¼Œå®ç° IAgentPresenter æ¥å£ |
| **agentLoopHandler** | `src/main/presenter/agentPresenter/loop/agentLoopHandler.ts` | 670 | Agent Loop ä¸»å¾ªç¯ï¼ˆwhile å¾ªç¯ï¼‰ |
| **streamGenerationHandler** | `src/main/presenter/agentPresenter/streaming/streamGenerationHandler.ts` | 645 | æµç”Ÿæˆåè°ƒï¼Œå‡†å¤‡ä¸Šä¸‹æ–‡ã€å¯åŠ¨ Loop |
| **loopOrchestrator** | `src/main/presenter/agentPresenter/loop/loopOrchestrator.ts` | ~30 | Loop çŠ¶æ€ç®¡ç†å™¨ |
| **toolCallProcessor** | `src/main/presenter/agentPresenter/loop/toolCallProcessor.ts` | 445 | å·¥å…·è°ƒç”¨æ‰§è¡Œå’Œç»“æœå¤„ç† |
| **llmEventHandler** | `src/main/presenter/agentPresenter/streaming/llmEventHandler.ts` | ~400 | æ ‡å‡†åŒ– LLM äº‹ä»¶ |
| **permissionHandler** | `src/main/presenter/agentPresenter/permission/permissionHandler.ts` | ~600 | æƒé™è¯·æ±‚å“åº”åè°ƒ |
| **messageBuilder** | `src/main/presenter/agentPresenter/message/messageBuilder.ts` | ~285 | æç¤ºè¯æ„å»º |
| **contentBufferHandler** | `src/main/presenter/agentPresenter/streaming/contentBufferHandler.ts` | ~200 | æµå¼å†…å®¹ç¼“å†²ä¼˜åŒ– |
| **toolCallHandler** | `src/main/presenter/agentPresenter/loop/toolCallHandler.ts` | ~500 | å·¥å…·è°ƒç”¨ UI å—ç®¡ç† |

## ğŸ—ï¸ æ¶æ„å…³ç³»

```mermaid
graph TB
    subgraph "AgentPresenter ä¸»å…¥å£"
        AgentP[AgentPresenter<br/>472è¡Œ]
    end

    subgraph "Agent Loop æ‰§è¡Œå±‚"
        StreamGen[streamGenerationHandler<br/>645è¡Œ]
        AgentLoop[agentLoopHandler<br/>670è¡Œ]
        LoopOrch[loopOrchestrator]
        ToolCallProc[toolCallProcessor<br/>445è¡Œ]
    end

    subgraph "äº‹ä»¶å¤„ç†å±‚"
        LLMEvent[llmEventHandler<br/>~400è¡Œ]
        ToolCall[toolCallHandler<br/>~500è¡Œ]
        BufHandler[contentBufferHandler<br/>~200è¡Œ]
    end

    subgraph "è¾…åŠ©ç»„ä»¶"
        MessageBuilder[messageBuilder<br/>~285è¡Œ]
        PermHandler[permissionHandler<br/>~600è¡Œ]
        Utility[utilityHandler]
    end

    AgentP --> StreamGen
    AgentP --> PermHandler
    AgentP --> Utility

    StreamGen --> AgentLoop
    StreamGen --> LLMEvent
    StreamGen --> MessageBuilder

    AgentLoop --> LoopOrch
    AgentLoop --> ToolCallProc
    AgentLoop --> ToolCall

    ToolCallProc --> AgentP
    ToolCall --> LLMEvent
    LLMEvent --> BufHandler

    LoopOrch --> LLMEvent

    classDef entry fill:#e3f2fd
    classDef loop fill:#fff3e0
    classDef event fill:#f3e5f5
    classDef util fill:#e8f5e9

    class AgentP entry
    class StreamGen,AgentLoop,LoopOrch,ToolCallProc loop
    class LLMEvent,ToolCall,BufHandler event
    class MessageBuilder,PermHandler,Utility util
```

## ğŸ¯ AgentPresenter ä¸»å…¥å£

### æ ¸å¿ƒæ–¹æ³•

```typescript
class AgentPresenter implements IAgentPresenter {
  // 1. å‘é€æ¶ˆæ¯ï¼ˆå¯åŠ¨æ–°çš„ Agent Loopï¼‰
  async sendMessage(
    agentId: string,
    content: string,
    tabId?: number,
    selectedVariantsMap?: Record<string, string>
  ): Promise<AssistantMessage | null>

  // 2. ç»§ç»­ç”Ÿæˆï¼ˆä»æ–­ç‚¹æ¢å¤ï¼‰
  async continueLoop(
    agentId: string,
    messageId: string,
    selectedVariantsMap?: Record<string, string>
  ): Promise<AssistantMessage | null>

  // 3. å–æ¶ˆç”Ÿæˆ
  async cancelLoop(messageId: string): Promise<void>

  // 4. é‡è¯•æ¶ˆæ¯
  async retryMessage(
    messageId: string,
    selectedVariantsMap?: Record<string, string>
  ): Promise<AssistantMessage>

  // 5. ä»ç”¨æˆ·æ¶ˆæ¯é‡æ–°ç”Ÿæˆ
  async regenerateFromUserMessage(
    agentId: string,
    userMessageId: string,
    selectedVariantsMap?: Record<string, string>
  ): Promise<AssistantMessage>

  // 6. ç¿»è¯‘æ–‡æœ¬
  async translateText(text: string, tabId: number): Promise<string>

  // 7. AI é—®ç­”
  async askAI(text: string, tabId: number): Promise<string>

  // 8. æƒé™å“åº”
  async handlePermissionResponse(
    messageId: string,
    toolCallId: string,
    granted: boolean,
    permissionType: 'read' | 'write' | 'all' | 'command',
    remember?: boolean
  ): Promise<void>

  // 9. è·å–è¯·æ±‚é¢„è§ˆ
  async getMessageRequestPreview(agentId: string, messageId?: string): Promise<unknown>
}
```

**æ–‡ä»¶ä½ç½®**ï¼š`src/main/presenter/agentPresenter/index.ts:139-365`

### sendMessage æµç¨‹è¯¦è§£

```typescript
async sendMessage(agentId, content, tabId, selectedVariantsMap) {
  // 1. åˆ›å»ºç”¨æˆ·æ¶ˆæ¯
  const userMessage = await messageManager.sendMessage(
    agentId,
    content,
    'user',
    '',
    false,
    this.buildMessageMetadata(conversation)
  )

  // 2. åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯ï¼ˆåˆå§‹ä¸ºç©ºï¼‰
  const assistantMessage = await streamGenerationHandler.generateAIResponse(
    agentId,
    userMessage.id
  )

  // 3. è·Ÿè¸ªç”ŸæˆçŠ¶æ€
  this.trackGeneratingMessage(assistantMessage, agentId)

  // 4. æ›´æ–°ä¼šè¯çŠ¶æ€
  await this.updateConversationAfterUserMessage(agentId)

  // 5. å¯åŠ¨ Agent Loop
  await sessionManager.startLoop(agentId, assistantMessage.id)

  // 6. å¯åŠ¨æµç”Ÿæˆ
  void StreamGenerationHandler.startStreamCompletion(
    agentId,
    assistantMessage.id,
    selectedVariantsMap
  )

  return assistantMessage
}
```

**æ–‡ä»¶ä½ç½®**ï¼š`src/main/presenter/agentPresenter/index.ts:139-176`

### ç”ŸæˆçŠ¶æ€è·Ÿè¸ª

```typescript
private trackGeneratingMessage(message: AssistantMessage, conversationId: string) {
  this.generatingMessages.set(message.id, {
    message,
    conversationId,
    startTime: Date.now(),
    firstTokenTime: null,
    promptTokens: 0,
    reasoningStartTime: null,
    reasoningEndTime: null,
    lastReasoningTime: null
  })
}
```

## ğŸ”„ agentLoopHandler - Agent Loop ä¸»å¾ªç¯

### æ ¸å¿ƒç»“æ„

```typescript
async *startStreamCompletion(
  providerId: string,
  initialMessages: ChatMessage[],
  modelId: string,
  eventId: string,
  temperature: number,
  maxTokens: number,
  enabledMcpTools?: string[],
  thinkingBudget?: number,
  reasoningEffort?: 'minimal'|'low'|'medium'|'high',
  verbosity?: 'low'|'medium'|'high',
  enableSearch?: boolean,
  forcedSearch?: boolean,
  searchStrategy?: 'turbo'|'max',
  conversationId?: string
): AsyncGenerator<LLMAgentEvent>
```

**æ–‡ä»¶ä½ç½®**ï¼š`src/main/presenter/agentPresenter/loop/agentLoopHandler.ts:145-668`

### Agent Loop ä¸»å¾ªç¯é€»è¾‘

```mermaid
flowchart TD
    Start([Agent Loop å¼€å§‹]) --> InitLoop[åˆå§‹åŒ–å¾ªç¯å˜é‡<br/>conversationMessages, needContinue, toolCallCount]
    InitLoop --> CheckAbort{ç”¨æˆ·æ˜¯å¦ä¸­æ–­?}
    CheckAbort -->|æ˜¯| EndLoop([Loop ç»“æŸ])
    CheckAbort -->|å¦| CheckMaxCalls{toolCallCount >= MAX?}

    CheckMaxCalls -->|æ˜¯| SendMax[å‘é€ maximum_tool_calls_reached äº‹ä»¶]
    SendMax --> EndLoop
    CheckMaxCalls -->|å¦| ResetNeedContinue[needContinue = false]

    ResetNeedContinue --> GetTools[è·å–å·¥å…·å®šä¹‰<br/>getAllToolDefinitions]
    GetTools --> CallLLM[è°ƒç”¨ provider.coreStream<br/>å¸¦ filteredToolDefs]

    CallLLM --> LoopEvents{éå† LLM Stream äº‹ä»¶}

    LoopEvents --> EventText{text äº‹ä»¶<br/>ç´¯ç§¯ currentContent}
    EventText --> LoopEvents

    LoopEvents --> EventReasoning{reasoning äº‹ä»¶<br/>ç´¯ç§¯ currentReasoning}
    EventReasoning --> LoopEvents

    LoopEvents --> EventToolStart{tool_call_start<br/>åˆå§‹åŒ– currentToolChunks}
    EventToolStart --> LoopEvents

    LoopEvents --> EventToolChunk{tool_call_chunk<br/>ç´¯ç§¯å‚æ•°å¢é‡}
    EventToolChunk --> LoopEvents

    LoopEvents --> EventToolEnd{tool_call_end}
    EventToolEnd --> IsACP{providerId == 'acp'?}

    IsACP -->|æ˜¯| SendACPResult[å‘é€ tool_call: 'end' äº‹ä»¶<br/>ACP å·²æ‰§è¡Œ]}
    IsACP -->|å¦| PushToolCall[å°†å·¥å…·è°ƒç”¨åŠ å…¥ currentToolCalls]
    PushToolCall --> LoopEvents

    LoopEvents --> EventPermission{permission äº‹ä»¶<br/>å‘é€æƒé™è¯·æ±‚}
    EventPermission --> ExitLoop[é€€å‡ºå¾ªç¯<br/>ç­‰å¾…ç”¨æˆ·å“åº”]
    ExitLoop --> EndLoop

    LoopEvents --> EventStop{stop äº‹ä»¶}
    EventStop --> CheckReason{stop_reason == 'tool_use'?}

    CheckReason -->|æ˜¯| SetContinue[needContinue = true]
    CheckReason -->|å¦| SetStop[needContinue = false]
    SetContinue --> LoopEvents
    SetStop --> LoopEvents

    LoopEvents -->|æ‰€æœ‰äº‹ä»¶å¤„ç†å®Œ| AddAssistant[æ·»åŠ  assistant æ¶ˆæ¯åˆ°ä¸Šä¸‹æ–‡]
    AddAssistant --> CheckNeedTool{needContinue && æœ‰å·¥å…·è°ƒç”¨?}

    CheckNeedTool -->|æ˜¯| ExecuteTools[æ‰§è¡Œå·¥å…·è°ƒç”¨<br/>ToolCallProcessor]
    ExecuteTools --> ProcessToolLoop{éå† toolCalls æ‰§è¡Œ}
    ProcessToolLoop --> SendToolEvents[å‘é€å·¥å…·æ‰§è¡Œäº‹ä»¶]
    SendToolEvents --> AddToolResult[æ·»åŠ å·¥å…·ç»“æœåˆ°ä¸Šä¸‹æ–‡]
    AddToolResult --> IncrementCount[toolCallCount++]
    IncrementCount --> CheckContinue2{needContinue?}

    CheckContinue2 -->|æ˜¯| InitLoop
    CheckContinue2 -->|å¦| EndLoop

    CheckNeedTool -->|å¦| EndLoop

    EndLoop --> SendFinalUsage[å‘é€æœ€ç»ˆ usage äº‹ä»¶]
    SendFinalUsage --> EndStream([å‘é€ END äº‹ä»¶])
```

**æ–‡ä»¶ä½ç½®**ï¼š`src/main/presenter/agentPresenter/loop/agentLoopHandler.ts:222-627`

### å…³é”®ä»£ç ç‰‡æ®µ

```typescript
// ä¸»å¾ªç¯
while (needContinueConversation) {
  if (abortController.signal.aborted) break

  if (toolCallCount >= MAX_TOOL_CALLS) {
    yield { type: 'response', data: { maximum_tool_calls_reached: true } }
    break
  }

  needContinueConversation = false
  let currentContent = ''
  let currentToolCalls = []

  // è·å–å·¥å…·å®šä¹‰
  const toolDefs = await this.getToolPresenter().getAllToolDefinitions({
    enabledMcpTools,
    chatMode,
    supportsVision: this.currentSupportsVision,
    agentWorkspacePath
  })
  const filteredToolDefs = await this.filterToolsForChatMode(toolDefs, chatMode, modelId)

  // è°ƒç”¨ LLM
  const stream = provider.coreStream(
    conversationMessages,
    modelId,
    modelConfig,
    temperature,
    maxTokens,
    filteredToolDefs
  )

  // å¤„ç†æµäº‹ä»¶
  for await (const chunk of stream) {
    switch (chunk.type) {
      case 'text':
        currentContent += chunk.content
        yield { type: 'response', data: { eventId, content: chunk.content } }
        break

      case 'tool_call_end':
        if (providerId === 'acp') {
          // ACP Provider ç›´æ¥è¿”å›æ‰§è¡Œç»“æœ
          yield { type: 'response', data: { eventId, tool_call: 'end', ...completeArgs } }
        } else {
          // é ACP éœ€è¦æœ¬åœ°æ‰§è¡Œ
          currentToolCalls.push({ id: chunk.tool_call_id, name, arguments: completeArgs })
        }
        break

      case 'stop':
        needContinueConversation = chunk.stop_reason === 'tool_use'
        break
    }
  }

  // æ·»åŠ  assistant æ¶ˆæ¯
  conversationMessages.push({ role: 'assistant', content: currentContent })

  // æ‰§è¡Œå·¥å…·è°ƒç”¨
  if (needContinueConversation && currentToolCalls.length > 0) {
    const processor = this.toolCallProcessor.process({...})
    while (true) {
      const { value, done } = await processor.next()
      if (done) {
        toolCallCount = value.toolCallCount
        needContinueConversation = value.needContinueConversation
        break
      }
      yield value
    }
  }
}
```

## ğŸŒŠ streamGenerationHandler - æµç”Ÿæˆåè°ƒ

### ä¸»è¦èŒè´£

1. **å‡†å¤‡å¯¹è¯ä¸Šä¸‹æ–‡** - è·å–ç”¨æˆ·æ¶ˆæ¯ã€å†å²æ¶ˆæ¯ã€å¤„ç†å˜ä½“é€‰æ‹©
2. **å¤„ç†ç”¨æˆ·æ¶ˆæ¯å†…å®¹** - æå– URLã€å›¾ç‰‡ç­‰
3. **æ‰§è¡Œæœç´¢**ï¼ˆå¦‚å¯ç”¨ï¼‰
4. **æ„å»ºæç¤ºè¯** - ä½¿ç”¨ messageBuilder
5. **å¯åŠ¨ LLM Stream** - è°ƒç”¨ llmProviderPresenter
6. **æ¶ˆè´¹æµ** - é€šè¿‡ loopOrchestrator

### startStreamCompletion æµç¨‹

```typescript
async startStreamCompletion(conversationId: string, queryMsgId?: string, selectedVariantsMap?) {
  // 1. è·å–ç”ŸæˆçŠ¶æ€
  const state = this.findGeneratingState(conversationId)

  // 2. å¯åŠ¨ Loop
  await sessionManager.startLoop(conversationId, state.message.id)

  // 3. å‡†å¤‡ä¼šè¯ä¸Šä¸‹æ–‡
  const { conversation, userMessage, contextMessages } = await this.prepareConversationContext(
    conversationId,
    queryMsgId,
    selectedVariantsMap
  )

  // 4. è§£æ workspace context
  const { chatMode, agentWorkspacePath } = await sessionManager.resolveWorkspaceContext(conversationId, modelId)

  // 5. å¤„ç†ç”¨æˆ·æ¶ˆæ¯å†…å®¹ï¼ˆURLã€å›¾ç‰‡ï¼‰
  const { userContent, urlResults, imageFiles } = await this.processUserMessageContent(userMessage)

  // 6. æ‰§è¡Œæœç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
  let searchResults = null
  if (userMessage.content.search) {
    searchResults = await this.searchHandler.startStreamSearch(conversationId, state.message.id, userContent)
  }

  // 7. æ„å»ºæç¤ºè¯
  const { finalContent, promptTokens } = await preparePromptContent({
    conversation,
    userContent,
    contextMessages,
    searchResults,
    urlResults,
    userMessage,
    vision: modelConfig?.vision,
    imageFiles,
    supportsFunctionCall: modelConfig.functionCall,
    modelType: modelConfig.type
  })

  // 8. å¯åŠ¨ LLM Stream
  const stream = llmProviderPresenter.startStreamCompletion(
    providerId,
    finalContent,
    modelId,
    eventId,
    temperature,
    maxTokens,
    enabledMcpTools,
    thinkingBudget,
    reasoningEffort,
    verbosity,
    enableSearch,
    forcedSearch,
    searchStrategy,
    conversationId
  )

  // 9. æ¶ˆè´¹æµ
  await this.loopOrchestrator.consume(stream)
}
```

**æ–‡ä»¶ä½ç½®**ï¼š`src/main/presenter/agentPresenter/streaming/streamGenerationHandler.ts:54-179`

### continueStreamCompletion - ç»§ç»­ç”Ÿæˆ

```typescript
async continueStreamCompletion(conversationId: string, queryMsgId: string, selectedVariantsMap?) {
  // 1. æ£€æŸ¥å¾…æ‰§è¡Œçš„å·¥å…·è°ƒç”¨ï¼ˆmaximum_tool_calls_reachedï¼‰
  const queryMessage = await this.ctx.messageManager.getMessage(queryMsgId)
  const content = queryMessage.content as AssistantMessageBlock[]
  const lastActionBlock = content.filter((block) => block.type === 'action').pop()

  if (lastActionBlock?.action_type === 'maximum_tool_calls_reached' && lastActionBlock.tool_call) {
    // 2. æ‰§è¡Œå·¥å…·è°ƒç”¨
    const toolCallResponse = await presenter.mcpPresenter.callTool({
      id: lastActionBlock.tool_call.id,
      type: 'function',
      function: {
        name: lastActionBlock.tool_call.name,
        arguments: lastActionBlock.tool_call.params
      },
      server: {
        name: lastActionBlock.tool_call.server_name,
        icons: lastActionBlock.tool_call.server_icons,
        description: lastActionBlock.tool_call.server_description
      }
    })

    // 3. å‘é€å·¥å…·æ‰§è¡Œäº‹ä»¶
    eventBus.sendToRenderer(STREAM_EVENTS.RESPONSE, SendTarget.ALL_WINDOWS, {
      eventId: state.message.id,
      tool_call: 'start',
      tool_call_id: toolCall.id,
      tool_call_name: toolCall.name,
      tool_call_params: toolCall.params,
      tool_call_response: toolCallResponse.content
    })
    // ... running, end äº‹ä»¶
  }

  // 4. å‡†å¤‡ä¸Šä¸‹æ–‡å¹¶ç»§ç»­
  const { conversation, contextMessages, userMessage } = await this.prepareConversationContext(...)
  const { finalContent } = await preparePromptContent({
    conversation,
    userContent: 'continue',  // ç‰¹æ®Šæ ‡è®°ç»§ç»­
    contextMessages,
    searchResults: null,
    ...
  })

  // 5. ç»§ç»­æµå¼ç”Ÿæˆ
  const stream = llmProviderPresenter.startStreamCompletion(...)
  await this.loopOrchestrator.consume(stream)
}
```

**æ–‡ä»¶ä½ç½®**ï¼š`src/main/presenter/agentPresenter/streaming/streamGenerationHandler.ts:181-350`

## ğŸ”„ loopOrchestrator - å¾ªç¯ç¼–æ’

```typescript
class LoopOrchestrator {
  constructor(private llmEventHandler: LLMEventHandler) {}

  async consume(stream: AsyncGenerator<LLMAgentEvent>) {
    for await (const event of stream) {
      if (event.type === 'response') {
        await this.llmEventHandler.handleResponse(event.data)
      } else if (event.type === 'error') {
        await this.llmEventHandler.handleError(event.data)
      } else if (event.type === 'end') {
        await this.llmEventHandler.handleEnd(event.data)
        break
      }
    }
  }
}
```

**æ–‡ä»¶ä½ç½®**ï¼š`src/main/presenter/agentPresenter/loop/loopOrchestrator.ts`

## ğŸ”§ toolCallProcessor - å·¥å…·è°ƒç”¨å¤„ç†

### ç»„ä»¶èŒè´£

```typescript
class ToolCallProcessor {
  // å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆå¼‚æ­¥ç”Ÿæˆå™¨ï¼‰
  async *process(context: {
    eventId: string
    toolCalls: Array<{id, name, arguments}>
    conversationMessages: ChatMessage[]
    modelConfig: any
    abortSignal: AbortSignal
    currentToolCallCount: number
    maxToolCalls: number
    conversationId: string
  }): AsyncGenerator<LLMAgentEvent>
}
```

**æ–‡ä»¶ä½ç½®**ï¼š`src/main/presenter/agentPresenter/loop/toolCallProcessor.ts:1-445`

### å¤„ç†æµç¨‹

```mermaid
sequenceDiagram
    participant Loop as Agent Loop
    participant TCP as toolCallProcessor
    participant TP as ToolPresenter
    participant EventBus as EventBus

    Loop->>TCP: process({toolCalls, eventId, ...})

    TCP->>TCP: æ£€æŸ¥å·¥å…·åˆ—è¡¨
    loop éå†æ¯ä¸ª toolCall
        TCP->>TP: callTool(toolCall)
        TP->>TP: ToolMapper è·¯ç”±

        alt MCP å·¥å…·
            TP->>TP: mcpPresenter.callTool()
        else Agent å·¥å…·
            TP->>TP: agentToolManager.callTool()
        end

        TP-->>TCP: toolResponse

        TCP->>EventBus: send {tool_call: 'running', ...}
        TCP->>EventBus: send {tool_call: 'end', toolResult}

        TCP->>TCP: æ·»åŠ  tool result åˆ°ä¸Šä¸‹æ–‡
        TCP-->>Loop: yield {type: 'response', data: {tool_call: 'end'}}

        TCP->>TCP: incrementToolCallCount()
        alt ç”¨æˆ·ä¸­æ–­
            TCP->>TCP: needContinueConversation = false
            break
        ToolCallCount>=MAX
            TCP->>TCP: needContinueConversation = false
            TCP-->>Loop: yield {maximum_tool_calls_reached: true}
            break
        end
    end

    TCP-->>Loop: return {toolCallCount, needContinueConversation}
```

**æ–‡ä»¶ä½ç½®**ï¼š`src/main/presenter/agentPresenter/loop/toolCallProcessor.ts`

## ğŸ“¡ llmEventHandler - äº‹ä»¶å¤„ç†

### æ ‡å‡†åŒ–äº‹ä»¶å¤„ç†

```typescript
class LLMEventHandler {
  async handleResponse(data: LLMAgentEvent['data']) {
    const { content, tool_call, tool_call_id, tool_call_name, tool_call_params } = data

    if (content) {
      await this.contentBufferHandler.accumulate(eventId, content)
    }

    if (tool_call) {
      await this.toolCallHandler.handleToolCallEvent(data)
    }
  }

  async handleError(data: {eventId, error}) {
    await this.messageManager.handleMessageError(eventId, error)
  }

  async handleEnd(data: {eventId, userStop}) {
    await this.contentBufferHandler.flush(eventId)
    await this.conversationUpdates(state)
  }
}
```

**æ–‡ä»¶ä½ç½®**ï¼š`src/main/presenter/agentPresenter/streaming/llmEventHandler.ts`

## ğŸ” permissionHandler - æƒé™åè°ƒ

### æƒé™å“åº”å¤„ç†

```typescript
async handlePermissionResponse(
  messageId: string,
  toolCallId: string,
  granted: boolean,
  permissionType: 'read' | 'write' | 'all' | 'command',
  remember?: boolean
) {
  const message = await this.getMessage(messageId)
  const content = message.content as AssistantMessageBlock[]

  // 1. æ›´æ–°æƒé™å—çŠ¶æ€
  const permissionBlock = content.find(
    block => block.type === 'action' && block.tool_call?.id === toolCallId
  )
  permissionBlock.status = granted ? 'granted' : 'denied'
  await this.ctx.messageManager.editMessage(messageId, JSON.stringify(content))

  // 2. æ¸…é™¤å¾…å¤„ç†æƒé™
  this.ctx.sessionManager.clearPendingPermission(message.conversationId)

  if (granted) {
    // 3. æ‰¹å‡†æƒé™
    if (isACPPermission) {
      await this.ctx.llmProviderPresenter.resolveAgentPermission(requestId, true)
    } else if (permissionType === 'command') {
      CommandPermissionService.approve(conversationId, signature, remember)
    } else {
      await this.ctx.mcpPresenter.grantPermission(serverName, permissionType, remember)
    }

    // 4. æ¢å¤ Agent Loop
    await this.ctx.sessionManager.startLoop(conversationId, messageId)
    await this.streamGenerationHandler.continueStreamCompletion(conversationId, messageId)
  } else {
    // 5. æ‹’ç»æƒé™
    await this.continueAfterPermissionDenied(messageId)
  }
}
```

**æ–‡ä»¶ä½ç½®**ï¼š`src/main/presenter/agentPresenter/permission/permissionHandler.ts`

## ğŸ› ï¸ messageBuilder - æç¤ºè¯æ„å»º

### ä¸»è¦æ–¹æ³•

```typescript
async function preparePromptContent(context: {
  conversation: CONVERSATION
  userContent: string
  contextMessages: Message[]
  searchResults?: SearchResult[]
  urlResults?: SearchResult[]
  userMessage: Message
  vision?: boolean
  imageFiles?: MessageFile[]
  supportsFunctionCall?: boolean
  modelType?: 'chat' | 'image' | 'audio'
}): Promise<{ finalContent: ChatMessage[], promptTokens: number }>
```

**æ–‡ä»¶ä½ç½®**ï¼š`src/main/presenter/agentPresenter/message/messageBuilder.ts`

### æ„å»ºæµç¨‹

```typescript
async function preparePromptContent(context) {
  const { conversation, userContent, contextMessages, searchResults, urlResults } = context

  // 1. åŸºç¡€æ¶ˆæ¯åˆ—è¡¨
  let messages = contextMessages.map(msg => ({
    role: msg.role,
    content: buildUserMessageContext(msg.content)
  }))

  // 2. æ·»åŠ ç³»ç»Ÿæç¤ºè¯
  const systemPrompt = buildSystemPrompt(conversation)
  messages.unshift({ role: 'system', content: systemPrompt })

  // 3. æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
  const userMessage = {
    role: 'user',
    content: buildUserMessageWithContext(userMessage.content, searchResults, urlResults)
  }
  messages.push(userMessage)

  // 4. å¤„ç†å›¾ç‰‡ï¼ˆvisionï¼‰
  if (conversation.settings.vision && context.imageFiles.length > 0) {
    userMessage.content = combineTextAndImages(userMessage.content, context.imageFiles)
  }

  // 5. å‹ç¼©ä¸Šä¸‹æ–‡ï¼ˆå¦‚è¶…è¿‡é™åˆ¶ï¼‰
  messages = await MessageCompressor.compress(messages, conversation.settings.contextLength)

  // 6. æ ¼å¼åŒ–ä¸º OpenAI æ ¼å¼
  const finalContent = toOpenAIMessages(messages)

  return { finalContent, promptTokens: calculateTokens(messages) }
}
```

**æ–‡ä»¶ä½ç½®**ï¼š`src/main/presenter/agentPresenter/message/messageBuilder.ts`

## ğŸ“Š contentBufferHandler - å†…å®¹ç¼“å†²

### ä¼˜åŒ–ç­–ç•¥

```typescript
class ContentBufferHandler {
  // ç´¯ç§¯å†…å®¹
  async accumulate(eventId: string, content: string) {
    const state = this.generatingMessages.get(eventId)
    if (!state) return

    // ç´¯ç§¯åˆ° adaptiveBuffer
    if (!state.adaptiveBuffer) {
      state.adaptiveBuffer = []
    }
    state.adaptiveBuffer.push({
      content,
      timestamp: Date.now(),
      size: content.length
    })

    // è‡ªé€‚åº”åˆ·æ–°
    const totalSize = state.adaptiveBuffer.reduce((sum, item) => sum + item.size, 0)
    if (totalSize >= this.threshold) {
      await this.flushAdaptiveBuffer(eventId)
    }
  }

  // åˆ·æ–°åˆ°å‰ç«¯
  async flushAdaptiveBuffer(eventId: string) {
    const state = this.generatingMessages.get(eventId)
    if (!state?.adaptiveBuffer) return

    const combined = state.adaptiveBuffer.map(item => item.content).join('')
    state.adaptiveBuffer = []

    await this.messageManager.editMessage(eventId, JSON.stringify(combined))
    eventBus.sendToRenderer(STREAM_EVENTS.RESPONSE, { eventId, content: combined })
  }
}
```

**æ–‡ä»¶ä½ç½®**ï¼š`src/main/presenter/agentPresenter/streaming/contentBufferHandler.ts`

## ğŸ“ å…³é”®æ–‡ä»¶ä½ç½®æ±‡æ€»

- **AgentPresenter**: `src/main/presenter/agentPresenter/index.ts:1-472`
- **agentLoopHandler**: `src/main/presenter/agentPresenter/loop/agentLoopHandler.ts:145-668`
- **streamGenerationHandler**: `src/main/presenter/agentPresenter/streaming/streamGenerationHandler.ts:54-645`
- **loopOrchestrator**: `src/main/presenter/agentPresenter/loop/loopOrchestrator.ts`
- **toolCallProcessor**: `src/main/presenter/agentPresenter/loop/toolCallProcessor.ts:1-445`
- **llmEventHandler**: `src/main/presenter/agentPresenter/streaming/llmEventHandler.ts`
- **permissionHandler**: `src/main/presenter/agentPresenter/permission/permissionHandler.ts`
- **messageBuilder**: `src/main/presenter/agentPresenter/message/messageBuilder.ts:1-285`
- **contentBufferHandler**: `src/main/presenter/agentPresenter/streaming/contentBufferHandler.ts`
- **toolCallHandler**: `src/main/presenter/agentPresenter/loop/toolCallHandler.ts`

## ğŸ“š ç›¸å…³é˜…è¯»

- [æ•´ä½“æ¶æ„æ¦‚è§ˆ](../ARCHITECTURE.md#agent-ç¼–æ’å™¨å±‚)
- [å·¥å…·ç³»ç»Ÿè¯¦è§£](../architecture/tool-system.md)
- [æ ¸å¿ƒæµç¨‹](../FLOWS.md#å‘é€æ¶ˆæ¯å®Œæ•´æµç¨‹)
- [ä¼šè¯ç®¡ç†è¯¦è§£](./session-management.md)
